{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cad0155",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported all libraries succesfully\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import svd, matrix_rank, norm\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, SpectralClustering, MeanShift\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from nltk.cluster import KMeansClusterer, euclidean_distance, cosine_distance\n",
    "print(\"Imported all libraries succesfully\")\n",
    "#np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4f9a47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully read reviews for: ['Canon_PowerShot_SD500', 'Canon_S100', 'Diaper_Champ', 'Hitachi_router', 'ipod', 'Linksys_Router', 'MicroMP3', 'Nokia_6600', 'norton']\n",
      "Succesfully selected top words and created their pseudo words\n",
      "Top 50 words: ['one', 'ipod', 'use', 'phone', 'get', 'router', 'camera', 'player', 'like', 'great', 'time', 'battery', 'work', 'problem', 'good', 'diaper', 'product', 'would', 'zen', 'also', 'computer', 'well', 'really', 'feature', 'quality', 'take', 'easy', 'even', 'thing', 'micro', 'first', 'need', 'used', 'want', 'much', 'better', 'creative', 'software', 'go', 'picture', 'little', 'bag', 'music', 'sound', 'buy', 'still', 'mp3', 'make', 'song', 'review']\n"
     ]
    }
   ],
   "source": [
    "# -- Step 1 -- Cleaning and pre-processing all reviews\n",
    "# Read raw text from the review files\n",
    "sentences = list()\n",
    "path = \"product_reviews/\"\n",
    "products = [\"Canon_PowerShot_SD500\", \"Canon_S100\", \"Diaper_Champ\", \"Hitachi_router\", \"ipod\", \"Linksys_Router\", \"MicroMP3\", \"Nokia_6600\", \"norton\"]\n",
    "\n",
    "# Split raw text in sentences\n",
    "for p in products:\n",
    "    f = open(path+p+'.txt', 'r')\n",
    "    for line in f:\n",
    "        if line.strip() != \"[t]\":\n",
    "            try:\n",
    "                sentences.append(line.strip().split(\"##\", 1)[1])\n",
    "            except IndexError:\n",
    "                # Some sentences don't have the \"##\" so skip them\n",
    "                pass\n",
    "                #sentences.append(line.strip().split(\"##\", 1)[0])\n",
    "\n",
    "fdist = FreqDist()\n",
    "token_sentences = list()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('u') # Decided to also remove this as a lot of people in the reviews used it instead of 'you'\n",
    "lem = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Tokenizing raw text in the sentences\n",
    "for sentence in sentences:\n",
    "    tokens = wordpunct_tokenize(sentence)\n",
    "    tokens_filtered = [t for t in tokens if t.lower() not in stop_words]\n",
    "    tokens_filtered = [t.lower() for t in tokens_filtered if t.lower().isalnum() and not t.lower().isnumeric()]\n",
    "    \n",
    "    tokens_filtered = [lem.lemmatize(t) for t in tokens_filtered]\n",
    "    \n",
    "    tokens_filtered = [t for t in tokens_filtered if len(t.lower()) > 1]\n",
    "    \n",
    "    token_sentences.append(tokens_filtered)\n",
    "    fdist.update(tokens_filtered)\n",
    "\n",
    "# Getting the target and pseudo words\n",
    "target_words = [x[0] for x in fdist.most_common(50)]\n",
    "pseudo_words = [x[::-1] for x in target_words]\n",
    "\n",
    "# Create position index for terms, first half with the target words, second half with the pseudo words\n",
    "terms = np.array(target_words+pseudo_words, dtype=object)\n",
    "position_index_init = {k:[] for k in terms}\n",
    "for t in target_words:\n",
    "    for i,s in enumerate(token_sentences):\n",
    "        for j,word in enumerate(s):\n",
    "            if t == word:\n",
    "                position_index_init[t].append((i, j))\n",
    "                \n",
    "print(\"Succesfully read reviews for: \" + str(products))\n",
    "print(\"Succesfully selected top words and created their pseudo words\")\n",
    "print(\"Top 50 words: \" + str(target_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "220e9892",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def construct_feature_vector(terms, vocab, documents, position_index):\n",
    "    \"\"\"\n",
    "    Create a word (terms) - feature (vocab in document) vector\n",
    "    Improved by use of the positional index of the terms\n",
    "    \"\"\"\n",
    "    context_labels = {terms[i] : i for i in range(0, len(terms))}\n",
    "    context_vocab = {vocab[i] : i for i in range(0, len(vocab))}\n",
    "    term_context = np.zeros((len(context_labels.keys()), len(context_vocab.keys())))\n",
    "    for key_word in position_index.keys():\n",
    "        for location in position_index[key_word]:\n",
    "            sentence = documents[location[0]]\n",
    "            for j in range(max(0, location[1] - window), min(len(sentence), location[1] + window+1)):\n",
    "                if location[1] == j:\n",
    "                    continue\n",
    "                term_context[context_labels[key_word]][context_vocab[stemmer.stem(sentence[j])]] += 1\n",
    "    index = np.argwhere(np.all(term_context[..., :] == 0, axis=0))\n",
    "    term_context_final = np.delete(term_context, index, axis=1)\n",
    "    return term_context_final, context_labels, context_vocab\n",
    "\n",
    "def run(testing_number_, window_, verbose_):\n",
    "# -- Step 6 -- Repeating the process testing_number times\n",
    "    testing_number = testing_number_\n",
    "    window = window_\n",
    "    verbose = verbose_\n",
    "    performance = []\n",
    "    timer = []\n",
    "    for testing in range(testing_number):\n",
    "        loop_time = time.time()\n",
    "        # -- Step 2 -- Sampeling half of the occurences in the corpus with the pseudo words --\n",
    "        # Create new temporary documents and position index for each iteration\n",
    "        documents = np.array(copy.deepcopy(token_sentences), dtype=object)\n",
    "        position_index = copy.deepcopy(position_index_init)\n",
    "\n",
    "        # Uniformly sample half of its occurances in the position index for each word pseudo\n",
    "        for k in position_index.keys():\n",
    "            pseudo = k[::-1]\n",
    "            if pseudo == list(position_index.keys())[0]:\n",
    "                break\n",
    "            pseudo_sample = np.random.choice(range(0, len(position_index[k])), size=int(len(position_index[k])/2), replace=False)\n",
    "            for i in pseudo_sample:\n",
    "                documents[position_index[k][i][0]][position_index[k][i][1]] = pseudo\n",
    "                position_index[pseudo].append([position_index[k][i][0], position_index[k][i][1]])\n",
    "                position_index[k][i] = \"x\"\n",
    "        # Deleting swapped indexed locations of the target words\n",
    "        for k in position_index.keys():\n",
    "            position_index[k] = [value for value in position_index[k] if value != \"x\"]\n",
    "\n",
    "        # -- Step 3 -- Constructing the feature context matrix --\n",
    "        # Adding all words in the documents including the pseudo_words to the vocabulary\n",
    "        vocab = list(fdist.keys()) + pseudo_words\n",
    "        vocab = [stemmer.stem(t) for t in vocab]\n",
    "        vocab = list(dict.fromkeys(vocab))\n",
    "\n",
    "        # Creating the feature context matrix based on the vocabulary and the indexed terms (labels)\n",
    "        term_context, context_labels, context_vocab = construct_feature_vector(terms, vocab, documents, position_index)\n",
    "\n",
    "        # -- Step 4 -- Applying clustering on the feature context matrix --\n",
    "        km = KMeans(n_clusters=50).fit(term_context)\n",
    "        labels = km.labels_\n",
    "        labels.tolist()\n",
    "\n",
    "        # -- Step 5 -- Checking clusters --\n",
    "        performance.append((np.sum(labels[0:50] == labels[50:100])) / len(labels[0:50]))\n",
    "        timer.append(time.time() - loop_time)\n",
    "        if verbose == True:\n",
    "            print(\"---- Labels for run \" + str(testing+1) +\" ----\")\n",
    "            print(\"Target: \" + str(labels[0:50]))\n",
    "            print(\"Pseudo: \" + str(labels[50:100]))\n",
    "            print(\"Performance: \" + str(performance[testing]))\n",
    "    return performance, timer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81d78e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Labels for run 1 ----\n",
      "Target: [42 10 36 22 23 13 11  9  6 35 30 14 19 43 25  8 25  5  7 25 45  0 25 25\n",
      " 20 48 15 25  4  2 25  6 25 25 27 39 28 24 41 21  5 40 49 21 25 25 12 25\n",
      " 25  1]\n",
      "Pseudo: [16 38 17 26 31 34 37 32  6 35 30 33 44 43 46  8 25 47 29 25  3  0  4 25\n",
      " 20 25 15 25 25 18 25 25 25 25 27 39 28 24 41 21 25 40 49 21 25 25 12 25\n",
      " 25  1]\n",
      "Performance: 0.6\n",
      "---- Labels for run 2 ----\n",
      "Target: [13  9 14 16 29 10 25 32  5 40 36  3 19 35 48  2 47  5  4 39 34 24  5  5\n",
      "  6 42 21 49 39  1  5 20 39 39 44 37 41 38  0 30 39 31 18 30  5 39 11 39\n",
      " 18 27]\n",
      "Pseudo: [46  8 26 23 22 17 12  7 28 15 18 45 19 35 33  2 47 43  4  5 39 24 39  5\n",
      "  6 39 21 49 39  1 39 20 20  5 44 37 41 38  0 30 39 31 18 30  5 39 11 39\n",
      " 18 27]\n",
      "Performance: 0.58\n",
      "---- Labels for run 3 ----\n",
      "Target: [24 12 27 16 35 13  9 46 29 31 33  7 40 34 41  5 44  0  1 22 37  3 22 22\n",
      " 19 22 15 43 22  4 22 22 17 22 28 47 20 45 49 14 22 18 32 23 22 22  8 22\n",
      " 32 30]\n",
      "Pseudo: [21  2 11 39 38 10 26  6 42 48 36  7 25 34  0  5 44 22  1 22 22  3 22 22\n",
      " 19 22 15 22 22  4 22 22 17  0 28 47 20 45 49 23 22 18 32 23 22 22  8 22\n",
      " 22 30]\n",
      "Performance: 0.62\n",
      "---- Labels for run 4 ----\n",
      "Target: [10 20 11 25 15 30 34 33 32 38 39 26 40 31 28  9 44 46  5  1 49  0  1  1\n",
      "  7  1 16 47 45  2  1  1 48  1 27  1 37 43  1 29  1 41 36 19  1  1  6  1\n",
      " 36 35]\n",
      "Pseudo: [17  8  4  3 24 21 14 12 22 38 48 13 18 42 28  9  1 23  5  1  1  0  1  1\n",
      "  7  1 16 47 45  2  1  1 48  1 27  1 37 43  1 29  1 41 36 19  1  1  6  1\n",
      " 43 35]\n",
      "Performance: 0.66\n",
      "---- Labels for run 5 ----\n",
      "Target: [ 8 33 17 20 36 46 12  6 23 37 48  2 27 47 22  4 40  5 10  5  5 25 41 41\n",
      " 13  5 19  1  5  3  5  5  5  5 34 26 24 45 39 42  5 31  5 11  5  5 16  5\n",
      "  5 30]\n",
      "Pseudo: [ 7 21 18 29 14  9 38 28 44 15  0  2 35  5 43 32 40 49 10  5  5 25 41  5\n",
      " 13  5 19  1  5  3  5  5  5  5 34 26 24 45 39 42  5 31  5 11  5  5 16  5\n",
      "  5 30]\n",
      "Performance: 0.66\n",
      "Info and hyperparameters:\n",
      "| Ran 5 times\n",
      "| Window size 1\n",
      "Performance:\n",
      "| 0.624  mean\n",
      "| 0.03200000000000003  std\n",
      "Time:\n",
      "| 7.344531774520874  total seconds\n",
      "| 1.4667454719543458  mean seconds for loops\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Testing number, window size and verbose parameters\n",
    "testing_number = 5\n",
    "window = 1\n",
    "verbose = True\n",
    "performance, timer = run(testing_number_=testing_number, window_=window, verbose_=verbose)\n",
    "\n",
    "# Displaying results\n",
    "print(\"Info and hyperparameters:\")\n",
    "print(\"| Ran \" + str(len(performance)) + \" times\")\n",
    "print(\"| Window size \" + str(window))\n",
    "print(\"Performance:\")\n",
    "print(\"| %s  mean\" % (np.mean(performance)))\n",
    "print(\"| %s  std\" % (np.std(performance)))\n",
    "print(\"Time:\")\n",
    "print(\"| %s  total seconds\" % (time.time() - start_time))\n",
    "print(\"| %s  mean seconds for loops\" % (np.mean(timer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb054e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
