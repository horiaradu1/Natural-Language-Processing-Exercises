{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29717676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported all libraries succesfully\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import svd, matrix_rank, norm\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, SpectralClustering, MeanShift\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from nltk.cluster import KMeansClusterer, euclidean_distance, cosine_distance\n",
    "print(\"Imported all libraries succesfully\")\n",
    "#np.random.seed(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6319057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully read reviews for: ['Canon_PowerShot_SD500', 'Canon_S100', 'Diaper_Champ', 'Hitachi_router', 'ipod', 'Linksys_Router', 'MicroMP3', 'Nokia_6600', 'norton']\n",
      "Succesfully selected top words and created their pseudo words\n",
      "Top 50 words: ['one', 'ipod', 'use', 'phone', 'get', 'router', 'camera', 'player', 'like', 'great', 'time', 'battery', 'work', 'problem', 'good', 'diaper', 'product', 'would', 'zen', 'also', 'computer', 'well', 'really', 'feature', 'quality', 'take', 'easy', 'even', 'thing', 'micro', 'first', 'need', 'used', 'want', 'much', 'better', 'creative', 'software', 'go', 'picture', 'little', 'bag', 'music', 'sound', 'buy', 'still', 'mp3', 'make', 'song', 'review']\n"
     ]
    }
   ],
   "source": [
    "# -- Step 1 -- Cleaning and pre-processing all reviews\n",
    "# Read raw text from the review files\n",
    "sentences = list()\n",
    "path = \"product_reviews/\"\n",
    "products = [\"Canon_PowerShot_SD500\", \"Canon_S100\", \"Diaper_Champ\", \"Hitachi_router\", \"ipod\", \"Linksys_Router\", \"MicroMP3\", \"Nokia_6600\", \"norton\"]\n",
    "\n",
    "# Split raw text in sentences\n",
    "for p in products:\n",
    "    f = open(path+p+'.txt', 'r')\n",
    "    for line in f:\n",
    "        if line.strip() != \"[t]\":\n",
    "            try:\n",
    "                sentences.append(line.strip().split(\"##\", 1)[1])\n",
    "            except IndexError:\n",
    "                # Some sentences don't have the \"##\" so skip them\n",
    "                pass\n",
    "                #sentences.append(line.strip().split(\"##\", 1)[0])\n",
    "\n",
    "fdist = FreqDist()\n",
    "token_sentences = list()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('u') # Decided to also remove this as a lot of people in the reviews used it instead of 'you'\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "# Tokenizing raw text in the sentences\n",
    "for sentence in sentences:\n",
    "    tokens = wordpunct_tokenize(sentence)\n",
    "    tokens_filtered = [t for t in tokens if t.lower() not in stop_words]\n",
    "    tokens_filtered = [t.lower() for t in tokens_filtered if t.lower().isalnum() and not t.lower().isnumeric()]\n",
    "    \n",
    "    tokens_filtered = [lem.lemmatize(t) for t in tokens_filtered]\n",
    "    \n",
    "    tokens_filtered = [t for t in tokens_filtered if len(t.lower()) > 1]\n",
    "    \n",
    "    token_sentences.append(tokens_filtered)\n",
    "    fdist.update(tokens_filtered)\n",
    "\n",
    "# Getting the target and pseudo words\n",
    "target_words = [x[0] for x in fdist.most_common(50)]\n",
    "pseudo_words = [x[::-1] for x in target_words]\n",
    "\n",
    "# Create position index for terms\n",
    "terms = np.array(target_words+pseudo_words, dtype=object)\n",
    "position_index_init = {k:[] for k in terms}\n",
    "for t in target_words:\n",
    "    for i,s in enumerate(token_sentences):\n",
    "        for j,word in enumerate(s):\n",
    "            if t == word:\n",
    "                position_index_init[t].append((i, j))\n",
    "                \n",
    "print(\"Succesfully read reviews for: \" + str(products))\n",
    "print(\"Succesfully selected top words and created their pseudo words\")\n",
    "print(\"Top 50 words: \" + str(target_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b50c54e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def construct_feature_vector(terms, vocab, documents, position_index, window):\n",
    "    context_labels = {terms[i] : i for i in range(0, len(terms))}\n",
    "    context_vocab = {vocab[i] : i for i in range(0, len(vocab))}\n",
    "    term_context = np.zeros((len(context_labels.keys()), len(context_vocab.keys())))\n",
    "    for key_word in position_index.keys():\n",
    "        for location in position_index[key_word]:\n",
    "            sentence = documents[location[0]]\n",
    "            for j in range(max(0, location[1] - window), min(len(sentence), location[1] + window + 1)):\n",
    "                if location[1] == j:\n",
    "                    continue\n",
    "                term_context[context_labels[key_word]][context_vocab[sentence[j]]] += 1\n",
    "    index = np.argwhere(np.all(term_context[..., :] == 0, axis=0))\n",
    "    term_context_final = np.delete(term_context, index, axis=1)\n",
    "    return term_context_final, context_labels, context_vocab\n",
    "\n",
    "def run(testing_number_, window_, verbose_):\n",
    "    # -- Step 6 -- Repeating the process testing_number times\n",
    "    testing_number = testing_number_\n",
    "    window = window_\n",
    "    verbose = verbose_\n",
    "    performance = []\n",
    "    timer = []\n",
    "    for testing in range(testing_number):\n",
    "        loop_time = time.time()\n",
    "        # -- Step 2 -- Sampeling half of the occurences in the corpus with the pseudo words --\n",
    "        # Create new temporary documents and position index for each iteration\n",
    "        documents = np.array(copy.deepcopy(token_sentences), dtype=object)\n",
    "        position_index = copy.deepcopy(position_index_init)\n",
    "\n",
    "        # Uniformly sample half of its occurances in the position index for each word pseudo\n",
    "        for k in position_index.keys():\n",
    "            pseudo = k[::-1]\n",
    "            if pseudo == list(position_index.keys())[0]:\n",
    "                break\n",
    "            pseudo_sample = np.random.choice(range(0, len(position_index[k])), size=int(len(position_index[k])/2), replace=False)\n",
    "            for i in pseudo_sample:\n",
    "                documents[position_index[k][i][0]][position_index[k][i][1]] = pseudo\n",
    "                position_index[pseudo].append((position_index[k][i][0], position_index[k][i][1]))\n",
    "                position_index[k][i] = \"x\"\n",
    "        # Deleting swapped indexed locations of the target words\n",
    "        for k in position_index.keys():\n",
    "            position_index[k] = [value for value in position_index[k] if value != \"x\"]\n",
    "\n",
    "        # -- Step 3 -- Constructing the feature context matrix --\n",
    "        # Adding all words in the documents including the pseudo_words to the vocabulary\n",
    "        vocab = list(fdist.keys()) + pseudo_words\n",
    "        vocab = list(dict.fromkeys(vocab))\n",
    "\n",
    "        # Creating the feature context matrix based on the vocabulary and the indexed terms (labels)\n",
    "        term_context, context_labels, context_vocab = construct_feature_vector(terms, vocab, documents, position_index, window)\n",
    "\n",
    "        # Apply LSI technique, with truncated SVD calculations and a normalizer\n",
    "        svd = TruncatedSVD(n_components=100)\n",
    "\n",
    "        normalizer = Normalizer(copy=False)\n",
    "        lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "        transformed = lsa.fit_transform(term_context)\n",
    "\n",
    "        # -- Step 4 -- Applying clustering on the feature context matrix --\n",
    "        km = KMeans(n_clusters=50).fit(transformed)\n",
    "        labels = km.labels_\n",
    "        labels.tolist()\n",
    "\n",
    "        # -- Step 5 -- Checking clusters --\n",
    "        performance.append((np.sum(labels[0:50] == labels[50:100])) / len(labels[0:50]))\n",
    "        timer.append(time.time() - loop_time)\n",
    "        if verbose == True:\n",
    "            print(\"---- Labels for run \" + str(testing+1) +\" ----\")\n",
    "            print(\"Target: \" + str(labels[0:50]))\n",
    "            print(\"Pseudo: \" + str(labels[50:100]))\n",
    "            print(\"Performance: \" + str(performance[testing]))\n",
    "    return performance, timer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9346716b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Labels for run 1 ----\n",
      "Target: [22  5 24 31  1 20 14  9 45  3 39 15  4 10 47 19 40 38  8 42  0 25 11 11\n",
      "  2 46 16 28 33  7 34 36 29 32 12 30  7 35 21 13 41 26  6 13 43 11 23 27\n",
      " 17 18]\n",
      "Pseudo: [22  5 24 31  1 20 14  9 45  3 39 15  4 10 47 19 48 38  8 44  0 25 33 11\n",
      "  2 46 16 28 33  7 34 36 29 37 12 30  7 35 21 13 23 26  6 13 43 49 23 27\n",
      " 17 18]\n",
      "Performance: 0.88\n",
      "---- Labels for run 2 ----\n",
      "Target: [23  5  9 21 48 37 24 15 14 22 49 10 17 11 22  8 32 18  4 46 26 12 45  0\n",
      " 13 36 16 42 25  3 29 21 43 40 28  2  3  1 38 41 19 33 44 41 31 27  7 47\n",
      " 35 20]\n",
      "Pseudo: [23  5  9 21 48 37 24 15 14 22 49 10 17 11 22  8 32 18  4 39 26 12 45  0\n",
      " 13 36 16 42 25  3 29 48 30 40 28  2  3  1 38  6 19 33 44 41 31 34  7 47\n",
      " 35 20]\n",
      "Performance: 0.9\n",
      "---- Labels for run 3 ----\n",
      "Target: [ 1  4 13 28  3 24 14  6  8 11 38 25 15 27 16 18 48 30 12 43 20 45 35  2\n",
      " 23 31 10 33 35  0 21 47 37 41 22 29  0 34 26  9 32 19  5  9 42  1  7 36\n",
      " 40 17]\n",
      "Pseudo: [ 1  4 13 28  3 24 14  6  8 11 38 25 15 27 16 18 48 30 12 10 20 45 35  2\n",
      " 23 31 10 33 35  0 21 47 37 49 22 29  0 34 26  9 44 19  5  9 42 46  7 36\n",
      " 39 17]\n",
      "Performance: 0.9\n",
      "---- Labels for run 4 ----\n",
      "Target: [25 17  4 30 42 14 21  8 38 16  7 18 34 24 44 12 39 35 15 32 43 16  9  9\n",
      "  3 31 20 27 47  2 33 42 22 48 23 36  2 11 19  6 41  5 10  6 49 37 13  0\n",
      " 28 26]\n",
      "Pseudo: [25 17  4 30 42 14 21  8 38  3  7 18 34 24 44 12 39 35 15 45 43 16  9  1\n",
      "  3 31 20 27 47  2 33 46 22 25 23 36  2 11 19  6 22  5 10  6 29 40 13  0\n",
      " 28 26]\n",
      "Performance: 0.84\n",
      "---- Labels for run 5 ----\n",
      "Target: [20 23 14 15 28  0 17 30  8 22 24  7 19 37  2  9  1 33 16 44 42 22  3 27\n",
      " 48 25 32 29 47  6 36  4 34 40 11 39  6 13 26 12 20 31 18 21 40 46  5 38\n",
      " 43 10]\n",
      "Pseudo: [20 23 14 15 28  0 17 30  8 22 24  7 19 37  2  9  1 33 16 44 42 22  3 27\n",
      " 48 25 32 29 47  6 36 49 34 41 11 39  6 13 26 12 45 31 18 21 40 35  5 38\n",
      " 43 10]\n",
      "Performance: 0.92\n",
      "Info and hyperparameters:\n",
      "| Ran 5 times\n",
      "| Window size 1\n",
      "Performance:\n",
      "| 0.8880000000000001  mean\n",
      "| 0.027129319932501096  std\n",
      "Time:\n",
      "| 5.091489791870117  total seconds\n",
      "| 1.0164679050445558  mean seconds for loops\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Testing number, window size and verbose parameters\n",
    "testing_number = 5\n",
    "window = 1\n",
    "verbose = True\n",
    "performance, timer = run(testing_number_=testing_number, window_=window, verbose_=verbose)\n",
    "\n",
    "# Displaying results\n",
    "print(\"Info and hyperparameters:\")\n",
    "print(\"| Ran \" + str(len(performance)) + \" times\")\n",
    "print(\"| Window size \" + str(window))\n",
    "print(\"Performance:\")\n",
    "print(\"| %s  mean\" % (np.mean(performance)))\n",
    "print(\"| %s  std\" % (np.std(performance)))\n",
    "print(\"Time:\")\n",
    "print(\"| %s  total seconds\" % (time.time() - start_time))\n",
    "print(\"| %s  mean seconds for loops\" % (np.mean(timer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3b6fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
