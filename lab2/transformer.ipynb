{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c9e30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in /home/horia/.local/lib/python3.8/site-packages (0.11.0)\r\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from torchtext) (2.22.0)\r\n",
      "Requirement already satisfied: numpy in /home/horia/.local/lib/python3.8/site-packages (from torchtext) (1.21.2)\r\n",
      "Requirement already satisfied: tqdm in /home/horia/.local/lib/python3.8/site-packages (from torchtext) (4.62.3)\r\n",
      "Requirement already satisfied: torch==1.10.0 in /home/horia/.local/lib/python3.8/site-packages (from torchtext) (1.10.0)\r\n",
      "Requirement already satisfied: typing-extensions in /home/horia/.local/lib/python3.8/site-packages (from torch==1.10.0->torchtext) (4.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a6267e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported all libraries succesfully\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.optim import SGD\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer, CrossEntropyLoss, Softmax\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "import torchtext\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "print(\"Imported all libraries succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a79cffd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully created Transformer Model class\n"
     ]
    }
   ],
   "source": [
    "# NEURAL NETWORK CLASSES\n",
    "\"\"\"\n",
    "Tutorial for pytorch followed from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "Accessed 10 December 2021\n",
    "\"\"\"\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\" Transformer \n",
    "        n_embeds: Number of embeddings/vocabulary\n",
    "        n_classes: Number of classes\n",
    "        d_model: Dimension of the model\n",
    "        n_head: Number of attention heads\n",
    "        d_hid: Dimension of the hidden layer\n",
    "        n_layers: Number of encoder layers\n",
    "        dropout: For training to zero elements randomly and prevent overfitting\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embeds, n_classes, d_model, n_head, d_hid, n_layers, dropout):\n",
    "        super().__init__()\n",
    "#         # Set model type\n",
    "#         self.model_type = 'Transformer'\n",
    "        # Get positional encoder\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # Get transformer encoder layers\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, n_head, d_hid, dropout)\n",
    "        \n",
    "        # Get transformer encoder\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, n_layers)\n",
    "        \n",
    "        # Embedding number and model\n",
    "        self.encoder = nn.Embedding(n_embeds, d_model)\n",
    "        \n",
    "        # Set model for transformer\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Set decoder\n",
    "        self.decoder = nn.Linear(d_model, n_classes)\n",
    "\n",
    "    # Initialize weights\n",
    "    def set_weights(self):\n",
    "        if isinstance(self, nn.Linear):\n",
    "            # Fill the tensor with uniform distribution values from Glorot initialization\n",
    "            nn.init.xavier_uniform(self.weight)\n",
    "            # Add a bias to those weights\n",
    "            self.bias.data.fill(0.005)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = output.mean(dim=0)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\" Positional Encoding \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class Collate(object):\n",
    "    \"\"\" Class to compute batching \"\"\"\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        batch_texts = list()\n",
    "        class_labels = list()\n",
    "        for (t, l) in batch:\n",
    "            processed_text = torch.tensor(self.pipeline(t, self.vocab), dtype=torch.int64)\n",
    "            batch_texts.append(processed_text)\n",
    "            class_labels.append(l)\n",
    "        \n",
    "        batch_texts = pad_sequence(batch_texts, batch_first=False, padding_value=self.vocab[\"<pad>\"])\n",
    "        class_labels = torch.tensor(class_labels, dtype=torch.int64)\n",
    "        return batch_texts, class_labels\n",
    "\n",
    "    def pipeline(self, text, vocab):\n",
    "        tokenized_text = [vocab[word] for word in wordpunct_tokenize(text)]\n",
    "        enclosed_text = [vocab[\"<bos>\"], *tokenized_text, vocab[\"<eos>\"]]\n",
    "        return enclosed_text\n",
    "\n",
    "print(\"Succesfully created Transformer Model class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a300ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMER COMPUTATION FUNCTIONS\n",
    "def batch_iter(model, batch, correct, loss, criterion):\n",
    "    sample = batch[0].to(device)\n",
    "    index = batch[1].to(device)\n",
    "    \n",
    "    # Process the data through the transformer \n",
    "    classifications = model(sample)\n",
    "\n",
    "    # Check how many correct results we get to get accuracy\n",
    "    output = torch.softmax(classifications, dim=1)\n",
    "    for i in range(len(index)):\n",
    "        if output[i].argmax() == index[i]:\n",
    "            correct += 1\n",
    "    \n",
    "    # Calculate loss function for specific batch\n",
    "    cs_loss = criterion(input=classifications, target=index)\n",
    "    loss += float(sample.shape[1] * cs_loss.item())\n",
    "    \n",
    "    return correct, loss, cs_loss\n",
    "\n",
    "def compute(model, dataloader, correct, loss, criterion, stochastic_gradient_descent, gradient_calculation):\n",
    "    # Disable or not gradient calculation to reduce memory consumption\n",
    "    if gradient_calculation:\n",
    "        # Go through each batch in the dataloader of train\n",
    "        for batch in dataloader:\n",
    "            stochastic_gradient_descent.zero_grad()\n",
    "            correct, loss, cs_loss = batch_iter(model, batch, correct, loss, criterion)\n",
    "            # Apply optimization and update weights based on the loss function\n",
    "            cs_loss.backward()\n",
    "            stochastic_gradient_descent.step()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                correct, loss, cs_loss = batch_iter(model, batch, correct, loss, criterion)\n",
    "        \n",
    "    return correct, loss\n",
    "\n",
    "def trainTransformerModel(transformer_model, epochs_number, data, vocab):\n",
    "    best_list = list(())\n",
    "    \"\"\"\n",
    "    lr - learning rate for the model in each SGD step\n",
    "    momentum - helps the gradient vectors in each step by smoothing it out\n",
    "    weight_decay - to penalyse the transformer training\n",
    "    \"\"\"\n",
    "    stochastic_gradient_descent = SGD(transformer_model.parameters(), lr=0.01, momentum=0.95, weight_decay=0.001)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    \n",
    "    # K-FOLD CROSS VALIDATION on the data\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    for i, (train_index, eval_index) in enumerate(kf.split(data)):\n",
    "        print(\"------\")\n",
    "        print(\"Fold : \" + str(i+1))\n",
    "        print(\"------\")\n",
    "        \n",
    "        init_time = time.time()\n",
    "        tr_performance = list()\n",
    "        eval_performance = list()\n",
    "        best_transformer = None\n",
    "        best_performance = (float(\"-inf\"), float(\"inf\"))\n",
    "        best_epoch = 0\n",
    "        \n",
    "        # Reset the weights of the model\n",
    "        transformer_model.set_weights()\n",
    "        learn_collate = Collate(vocab)\n",
    "        \n",
    "        train_subset = Subset(data, train_index)\n",
    "        eval_subset = Subset(data, eval_index)\n",
    "        \n",
    "        # Load respective data into dataloaders by batches to train and evaluate\n",
    "        train_dataloader = DataLoader(train_subset, batch_size=50, collate_fn=learn_collate, shuffle=True)\n",
    "        eval_dataloader = DataLoader(eval_subset, batch_size=50, collate_fn=learn_collate, shuffle=True)\n",
    "        \n",
    "        for epoch in range(epochs_number):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            #TRAINING\n",
    "            # -- Change pytorch transformer to training mode --\n",
    "            tr_correct = 0\n",
    "            tr_loss = 0\n",
    "            transformer_model.train()\n",
    "\n",
    "            # Compute training\n",
    "            tr_correct, tr_loss = compute(transformer_model, train_dataloader, tr_correct, tr_loss, criterion, stochastic_gradient_descent, True)\n",
    "\n",
    "            tr_correct = tr_correct/len(train_index)\n",
    "            tr_loss = tr_loss/len(train_index)\n",
    "            tr_performance.append((tr_correct, tr_loss))\n",
    "\n",
    "            # EVALUATION\n",
    "            # -- Change pytorch transformer to evaluation mode --\n",
    "            eval_correct = 0\n",
    "            eval_loss = 0\n",
    "            transformer_model.eval()\n",
    "\n",
    "            # Compute evaluation\n",
    "            eval_correct, eval_loss = compute(transformer_model, eval_dataloader, eval_correct, eval_loss, criterion, _, False)\n",
    "\n",
    "            eval_correct = eval_correct/len(eval_index)\n",
    "            eval_loss = eval_loss/len(eval_index)\n",
    "            eval_performance.append((eval_correct, eval_loss))\n",
    "\n",
    "            # FINDING BEST MODEL\n",
    "            # -- Evaluate epoch by loss function --\n",
    "            if ((best_performance[1] > eval_performance[epoch][1]) or ((best_performance[1] == eval_performance[epoch][1]) and (best_performance[0] < eval_performance[epoch][0]))):\n",
    "                best_performance = (eval_performance[epoch][0], eval_performance[epoch][1])\n",
    "                best_transformer = copy.deepcopy(transformer_model)\n",
    "                best_epoch = epoch\n",
    "\n",
    "            print(\"  For epoch : \" + str(epoch+1) + \" | Time : \" + str(round(time.time() - start_time,2)) + \" seconds\")\n",
    "            print(\"    Train   : performance= \" + str(round(tr_performance[epoch][0],2)) + \" | loss= \" + str(round(tr_performance[epoch][1],2)))\n",
    "            print(\"   Evaluate : performance= \" + str(round(eval_performance[epoch][0],2)) + \" | loss= \" + str(round(eval_performance[epoch][1],2)))\n",
    "            print(\"-----------------------------------------------------\")\n",
    "\n",
    "        print(\"-------------------------------------------------\")\n",
    "        print(\" Best epoch : \" + str(best_epoch+1) + \" | Total Time : \" + str(round(time.time() - init_time,2)))\n",
    "        print(\" With evaluation : performance= \" + str(round(best_performance[0],2)) + \" | loss= \" + str(round(best_performance[1],2)))\n",
    "    \n",
    "        # Add best transformer from each fold and their performance (accuracy, loss)\n",
    "        best_list.append((best_transformer, best_performance))\n",
    "    \n",
    "    return best_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f259406",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully extracted reviews as positive and negative as tuples in a list\n",
      "Found 2101 reviews\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPROCESSING AND CLEANING\n",
    "# Gettin the datasets from the reviews\n",
    "path = \"product_reviews/\"\n",
    "products = [\"Canon_PowerShot_SD500\", \"Canon_S100\", \"Diaper_Champ\", \"Hitachi_router\", \"ipod\", \"Linksys_Router\", \"MicroMP3\", \"Nokia_6600\", \"norton\"]\n",
    "\n",
    "# RegEx for the positive and negative sentences\n",
    "reviews = list()\n",
    "re_review = re.compile(r\"##(.*)$\")\n",
    "re_sentiment = re.compile(r\"((?:(?:\\w+\\s)+)?\\w+)\\[((?:\\+|\\-)(?:\\d|))\\]\")\n",
    "for p in products:\n",
    "    f = open(path+p+'.txt', 'r')\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Check if it is a review tag\n",
    "        if line.strip() == \"[t]\":\n",
    "            continue\n",
    "            \n",
    "        features = re.findall(re_sentiment, line)\n",
    "        # Check if there is any sentiment on the line\n",
    "        if len(features) == 0:\n",
    "            continue\n",
    "            \n",
    "        review = re.findall(re_review, line)\n",
    "        review = [r.lower() for r in review]\n",
    "        # Check that the review is properly read\n",
    "        if len(review) != 1:\n",
    "            # Some reviews (6, in MicroMP3) are not split properly with a ##\n",
    "            continue\n",
    "        \n",
    "        score = 0\n",
    "        # Get score for a review\n",
    "        for i in features:\n",
    "            try:\n",
    "                score += int(i[1])\n",
    "            except ValueError:\n",
    "                # Some reviews don't have a number, just a '+' or '-'\n",
    "                if i[1] == \"-\":\n",
    "                    score -= 1\n",
    "                else:\n",
    "                    score += 1\n",
    "        \n",
    "        # Reviews with a 0 score are also considered as positive\n",
    "        if score >= 0:\n",
    "            reviews.append((review[0], 1))\n",
    "        else:\n",
    "            reviews.append((review[0], 0))\n",
    "\n",
    "print(\"Succesfully extracted reviews as positive and negative as tuples in a list\")\n",
    "\n",
    "print(\"Found \" + str(len(reviews)) + \" reviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3231f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1891 for learning\n",
      "Using 210 for testing\n"
     ]
    }
   ],
   "source": [
    " # SPLITTING DATA in learning (for training and evaluation with k-fold cross) and testing\n",
    "test_size = int(0.1 * len(reviews)) #HYPERPARAMETER\n",
    "learn_size = len(reviews) - test_size\n",
    "learn_data, test_data = random_split(reviews, [learn_size, test_size])\n",
    "\n",
    "print(\"Using \" + str(len(learn_data)) + \" for learning\")\n",
    "print(\"Using \" + str(len(test_data)) + \" for testing\")\n",
    "\n",
    "# Tokenize dataset\n",
    "learn_tokens = [wordpunct_tokenize(sentence) for (sentence, _) in learn_data]\n",
    "\n",
    "# Using torchtext from pytorch, create a vocabulary for the dataset\n",
    "learn_vocab = torchtext.vocab.build_vocab_from_iterator(learn_tokens, specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "learn_vocab.set_default_index(learn_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03a96133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using  cuda  for processing, thus running  10 epochs\n",
      "\n",
      "------\n",
      "Fold : 1\n",
      "------\n",
      "  For epoch : 1 | Time : 0.66 seconds\n",
      "    Train   : performance= 0.61 | loss= 0.67\n",
      "   Evaluate : performance= 0.6 | loss= 1.01\n",
      "-----------------------------------------------------\n",
      "  For epoch : 2 | Time : 0.64 seconds\n",
      "    Train   : performance= 0.64 | loss= 0.67\n",
      "   Evaluate : performance= 0.61 | loss= 0.66\n",
      "-----------------------------------------------------\n",
      "  For epoch : 3 | Time : 0.63 seconds\n",
      "    Train   : performance= 0.64 | loss= 0.68\n",
      "   Evaluate : performance= 0.62 | loss= 0.67\n",
      "-----------------------------------------------------\n",
      "  For epoch : 4 | Time : 0.64 seconds\n",
      "    Train   : performance= 0.63 | loss= 0.69\n",
      "   Evaluate : performance= 0.6 | loss= 0.75\n",
      "-----------------------------------------------------\n",
      "  For epoch : 5 | Time : 0.63 seconds\n",
      "    Train   : performance= 0.66 | loss= 0.67\n",
      "   Evaluate : performance= 0.55 | loss= 0.72\n",
      "-----------------------------------------------------\n",
      "  For epoch : 6 | Time : 0.63 seconds\n",
      "    Train   : performance= 0.66 | loss= 0.65\n",
      "   Evaluate : performance= 0.64 | loss= 0.64\n",
      "-----------------------------------------------------\n",
      "  For epoch : 7 | Time : 0.64 seconds\n",
      "    Train   : performance= 0.71 | loss= 0.58\n",
      "   Evaluate : performance= 0.67 | loss= 0.63\n",
      "-----------------------------------------------------\n",
      "  For epoch : 8 | Time : 0.61 seconds\n",
      "    Train   : performance= 0.68 | loss= 0.62\n",
      "   Evaluate : performance= 0.67 | loss= 0.62\n",
      "-----------------------------------------------------\n",
      "  For epoch : 9 | Time : 0.64 seconds\n",
      "    Train   : performance= 0.69 | loss= 0.58\n",
      "   Evaluate : performance= 0.65 | loss= 0.65\n",
      "-----------------------------------------------------\n",
      "  For epoch : 10 | Time : 0.64 seconds\n",
      "    Train   : performance= 0.73 | loss= 0.53\n",
      "   Evaluate : performance= 0.66 | loss= 0.68\n",
      "-----------------------------------------------------\n",
      "-------------------------------------------------\n",
      " Best epoch : 8 | Total Time : 6.37\n",
      " With evaluation : performance= 0.67 | loss= 0.62\n",
      "------\n",
      "Fold : 2\n",
      "------\n",
      "  For epoch : 1 | Time : 0.64 seconds\n",
      "    Train   : performance= 0.73 | loss= 0.54\n",
      "   Evaluate : performance= 0.75 | loss= 0.49\n",
      "-----------------------------------------------------\n",
      "  For epoch : 2 | Time : 0.65 seconds\n",
      "    Train   : performance= 0.71 | loss= 0.56\n",
      "   Evaluate : performance= 0.72 | loss= 0.55\n",
      "-----------------------------------------------------\n",
      "  For epoch : 3 | Time : 0.63 seconds\n",
      "    Train   : performance= 0.73 | loss= 0.53\n",
      "   Evaluate : performance= 0.72 | loss= 0.57\n",
      "-----------------------------------------------------\n",
      "  For epoch : 4 | Time : 0.63 seconds\n",
      "    Train   : performance= 0.7 | loss= 0.55\n",
      "   Evaluate : performance= 0.72 | loss= 0.54\n",
      "-----------------------------------------------------\n",
      "  For epoch : 5 | Time : 0.63 seconds\n",
      "    Train   : performance= 0.75 | loss= 0.5\n",
      "   Evaluate : performance= 0.75 | loss= 0.52\n",
      "-----------------------------------------------------\n",
      "  For epoch : 6 | Time : 0.64 seconds\n",
      "    Train   : performance= 0.76 | loss= 0.48\n",
      "   Evaluate : performance= 0.75 | loss= 0.56\n",
      "-----------------------------------------------------\n",
      "  For epoch : 7 | Time : 0.64 seconds\n",
      "    Train   : performance= 0.77 | loss= 0.48\n",
      "   Evaluate : performance= 0.68 | loss= 0.66\n",
      "-----------------------------------------------------\n",
      "  For epoch : 8 | Time : 0.63 seconds\n",
      "    Train   : performance= 0.74 | loss= 0.53\n",
      "   Evaluate : performance= 0.74 | loss= 0.57\n",
      "-----------------------------------------------------\n",
      "  For epoch : 9 | Time : 0.6 seconds\n",
      "    Train   : performance= 0.77 | loss= 0.48\n",
      "   Evaluate : performance= 0.7 | loss= 0.85\n",
      "-----------------------------------------------------\n",
      "  For epoch : 10 | Time : 0.65 seconds\n",
      "    Train   : performance= 0.78 | loss= 0.47\n",
      "   Evaluate : performance= 0.74 | loss= 0.56\n",
      "-----------------------------------------------------\n",
      "-------------------------------------------------\n",
      " Best epoch : 1 | Total Time : 6.34\n",
      " With evaluation : performance= 0.75 | loss= 0.49\n",
      "------\n",
      "Fold : 3\n",
      "------\n",
      "  For epoch : 1 | Time : 0.62 seconds\n",
      "    Train   : performance= 0.78 | loss= 0.47\n",
      "   Evaluate : performance= 0.83 | loss= 0.37\n",
      "-----------------------------------------------------\n",
      "  For epoch : 2 | Time : 0.6 seconds\n",
      "    Train   : performance= 0.77 | loss= 0.45\n",
      "   Evaluate : performance= 0.81 | loss= 0.43\n",
      "-----------------------------------------------------\n",
      "  For epoch : 3 | Time : 0.62 seconds\n",
      "    Train   : performance= 0.79 | loss= 0.44\n",
      "   Evaluate : performance= 0.84 | loss= 0.4\n",
      "-----------------------------------------------------\n",
      "  For epoch : 4 | Time : 0.63 seconds\n",
      "    Train   : performance= 0.78 | loss= 0.45\n",
      "   Evaluate : performance= 0.82 | loss= 0.41\n",
      "-----------------------------------------------------\n",
      "  For epoch : 5 | Time : 0.64 seconds\n",
      "    Train   : performance= 0.81 | loss= 0.41\n",
      "   Evaluate : performance= 0.79 | loss= 0.46\n",
      "-----------------------------------------------------\n",
      "  For epoch : 6 | Time : 0.62 seconds\n",
      "    Train   : performance= 0.83 | loss= 0.39\n",
      "   Evaluate : performance= 0.81 | loss= 0.51\n",
      "-----------------------------------------------------\n",
      "  For epoch : 7 | Time : 0.61 seconds\n",
      "    Train   : performance= 0.83 | loss= 0.38\n",
      "   Evaluate : performance= 0.79 | loss= 0.5\n",
      "-----------------------------------------------------\n",
      "  For epoch : 8 | Time : 0.63 seconds\n",
      "    Train   : performance= 0.82 | loss= 0.4\n",
      "   Evaluate : performance= 0.8 | loss= 0.5\n",
      "-----------------------------------------------------\n",
      "  For epoch : 9 | Time : 0.61 seconds\n",
      "    Train   : performance= 0.83 | loss= 0.38\n",
      "   Evaluate : performance= 0.79 | loss= 0.51\n",
      "-----------------------------------------------------\n",
      "  For epoch : 10 | Time : 0.61 seconds\n",
      "    Train   : performance= 0.84 | loss= 0.38\n",
      "   Evaluate : performance= 0.76 | loss= 0.6\n",
      "-----------------------------------------------------\n",
      "-------------------------------------------------\n",
      " Best epoch : 1 | Total Time : 6.2\n",
      " With evaluation : performance= 0.83 | loss= 0.37\n",
      "------\n",
      "Fold : 4\n",
      "------\n",
      "  For epoch : 1 | Time : 0.63 seconds\n",
      "    Train   : performance= 0.8 | loss= 0.43\n",
      "   Evaluate : performance= 0.87 | loss= 0.31\n",
      "-----------------------------------------------------\n",
      "  For epoch : 2 | Time : 0.65 seconds\n",
      "    Train   : performance= 0.82 | loss= 0.4\n",
      "   Evaluate : performance= 0.86 | loss= 0.35\n",
      "-----------------------------------------------------\n",
      "  For epoch : 3 | Time : 0.64 seconds\n",
      "    Train   : performance= 0.79 | loss= 0.45\n",
      "   Evaluate : performance= 0.8 | loss= 0.42\n",
      "-----------------------------------------------------\n",
      "  For epoch : 4 | Time : 0.64 seconds\n",
      "    Train   : performance= 0.83 | loss= 0.39\n",
      "   Evaluate : performance= 0.82 | loss= 0.39\n",
      "-----------------------------------------------------\n",
      "  For epoch : 5 | Time : 0.64 seconds\n",
      "    Train   : performance= 0.82 | loss= 0.38\n",
      "   Evaluate : performance= 0.83 | loss= 0.37\n",
      "-----------------------------------------------------\n",
      "  For epoch : 6 | Time : 0.64 seconds\n",
      "    Train   : performance= 0.87 | loss= 0.3\n",
      "   Evaluate : performance= 0.83 | loss= 0.41\n",
      "-----------------------------------------------------\n",
      "  For epoch : 7 | Time : 0.63 seconds\n",
      "    Train   : performance= 0.86 | loss= 0.32\n",
      "   Evaluate : performance= 0.8 | loss= 0.43\n",
      "-----------------------------------------------------\n",
      "  For epoch : 8 | Time : 0.64 seconds\n",
      "    Train   : performance= 0.87 | loss= 0.31\n",
      "   Evaluate : performance= 0.82 | loss= 0.41\n",
      "-----------------------------------------------------\n",
      "  For epoch : 9 | Time : 0.62 seconds\n",
      "    Train   : performance= 0.89 | loss= 0.26\n",
      "   Evaluate : performance= 0.8 | loss= 0.56\n",
      "-----------------------------------------------------\n",
      "  For epoch : 10 | Time : 0.63 seconds\n",
      "    Train   : performance= 0.89 | loss= 0.28\n",
      "   Evaluate : performance= 0.78 | loss= 0.5\n",
      "-----------------------------------------------------\n",
      "-------------------------------------------------\n",
      " Best epoch : 1 | Total Time : 6.35\n",
      " With evaluation : performance= 0.87 | loss= 0.31\n",
      "------\n",
      "Fold : 5\n",
      "------\n",
      "  For epoch : 1 | Time : 0.61 seconds\n",
      "    Train   : performance= 0.84 | loss= 0.37\n",
      "   Evaluate : performance= 0.93 | loss= 0.19\n",
      "-----------------------------------------------------\n",
      "  For epoch : 2 | Time : 0.61 seconds\n",
      "    Train   : performance= 0.87 | loss= 0.31\n",
      "   Evaluate : performance= 0.92 | loss= 0.2\n",
      "-----------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  For epoch : 3 | Time : 0.63 seconds\n",
      "    Train   : performance= 0.87 | loss= 0.3\n",
      "   Evaluate : performance= 0.92 | loss= 0.22\n",
      "-----------------------------------------------------\n",
      "  For epoch : 4 | Time : 0.6 seconds\n",
      "    Train   : performance= 0.89 | loss= 0.27\n",
      "   Evaluate : performance= 0.88 | loss= 0.28\n",
      "-----------------------------------------------------\n",
      "  For epoch : 5 | Time : 0.65 seconds\n",
      "    Train   : performance= 0.88 | loss= 0.27\n",
      "   Evaluate : performance= 0.89 | loss= 0.27\n",
      "-----------------------------------------------------\n",
      "  For epoch : 6 | Time : 0.61 seconds\n",
      "    Train   : performance= 0.89 | loss= 0.26\n",
      "   Evaluate : performance= 0.9 | loss= 0.24\n",
      "-----------------------------------------------------\n",
      "  For epoch : 7 | Time : 0.61 seconds\n",
      "    Train   : performance= 0.9 | loss= 0.22\n",
      "   Evaluate : performance= 0.89 | loss= 0.31\n",
      "-----------------------------------------------------\n",
      "  For epoch : 8 | Time : 0.61 seconds\n",
      "    Train   : performance= 0.88 | loss= 0.27\n",
      "   Evaluate : performance= 0.87 | loss= 0.31\n",
      "-----------------------------------------------------\n",
      "  For epoch : 9 | Time : 0.6 seconds\n",
      "    Train   : performance= 0.9 | loss= 0.24\n",
      "   Evaluate : performance= 0.88 | loss= 0.33\n",
      "-----------------------------------------------------\n",
      "  For epoch : 10 | Time : 0.62 seconds\n",
      "    Train   : performance= 0.91 | loss= 0.22\n",
      "   Evaluate : performance= 0.87 | loss= 0.4\n",
      "-----------------------------------------------------\n",
      "-------------------------------------------------\n",
      " Best epoch : 1 | Total Time : 6.16\n",
      " With evaluation : performance= 0.93 | loss= 0.19\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Overall best accuracy and loss of the models from the 5-fold cross validation is\n",
      "Accuracy : 0.9285714285714286 |  Loss : 0.18659607955703028\n"
     ]
    }
   ],
   "source": [
    "# LEARNING (TRAINING AND EVALUATING)\n",
    "# GPU CUDA is a lot faster than CPU\n",
    "# If CPU, will run less epochs but will result in a much lower performance\n",
    "# Can change number below if it is to slow\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    epochs_number = 10 #HYPERPARAMETER\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    epochs_number = 5 #HYPERPARAMETER\n",
    "#epochs_number = 5\n",
    "print(\"Using  \" + str(device) + \"  for processing, thus running  \" + str(epochs_number) + \" epochs\\n\")\n",
    "\n",
    "\n",
    "#HYPERPARAMETERS for transformers\n",
    "transformer = TransformerModel(n_embeds=len(learn_vocab), n_classes=2, d_model=400, n_head=2, d_hid=400, n_layers=2, dropout=0.2).to(device)\n",
    "\n",
    "best_transformers = trainTransformerModel(transformer, epochs_number=epochs_number, data=learn_data, vocab=learn_vocab)\n",
    "\n",
    "\n",
    "score = (float(\"-inf\"), float(\"inf\"))\n",
    "winner = None\n",
    "for (model, (accuracy, loss)) in best_transformers:\n",
    "    if ((score[1] > loss) or ((score[1] == loss) and (score[0] < accuracy))):\n",
    "        score = (accuracy, loss)\n",
    "        winner = model\n",
    "    \n",
    "print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "print(\"Overall best accuracy and loss of the models from the 5-fold cross validation is\")\n",
    "print(\"Accuracy : \" + str(score[0]) + \" |  Loss : \" + str(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b22abbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++\n",
      "TEST RESULTS\n",
      " Accuracy : 0.7285714285714285\n",
      "   Loss   : 0.6513221604483468\n",
      "++++++++++++\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "# Now load the test data\n",
    "test_collator = Collate(learn_vocab)\n",
    "test_dataloader = DataLoader(test_data, batch_size=50, collate_fn=test_collator)\n",
    "# Do final test with best model\n",
    "winner.eval()\n",
    "correct = 0\n",
    "loss = 0\n",
    "criterion = CrossEntropyLoss()\n",
    "correct, loss = compute(winner, test_dataloader, correct, loss, criterion, _, False)\n",
    "correct = correct/test_size\n",
    "loss = loss/test_size\n",
    "\n",
    "print(\"++++++++++++\")\n",
    "print(\"TEST RESULTS\")\n",
    "print(\" Accuracy : \" + str(correct))\n",
    "print(\"   Loss   : \" + str(loss))\n",
    "print(\"++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcbb1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
