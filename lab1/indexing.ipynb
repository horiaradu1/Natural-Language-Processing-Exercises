{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader, stopwords\n",
    "from nltk import WordPunctTokenizer, TreebankWordTokenizer, FreqDist, word_tokenize, WhitespaceTokenizer, MWETokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "from nltk.util import ngrams\n",
    "import csv\n",
    "\n",
    "class InvertedIndex:\n",
    "    \"\"\"\n",
    "    Construct Inverted Index\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # The dictionaries to hold the data of the inverted index\n",
    "        self.inverted_index_docfreq = FreqDist()    # Store the inverted index term document frequency in a \n",
    "                                                    # frequency distribution from NLTK for easier use\n",
    "        self.inverted_index_postings = dict(dict()) # A nested dictionary for words and then their appereance\n",
    "                                                    # in each document by id {word:{docID:nr_of_apperanes}}\n",
    "        self.token_sequence = dict(list())       # Dictionary with docIDs as keys and the tokens in correct sequence\n",
    "                                                 # for proximity searches\n",
    "        self.documents = {}                      # Dictionary with the indexes as keys and the documents as text\n",
    "        \n",
    "        # Lists to keep track of characters and locations\n",
    "        self.characters = []\n",
    "        self.locations = []                      \n",
    "        \n",
    "        # Multiple types of tokenizers\n",
    "        self.tkn_p = WordPunctTokenizer()\n",
    "        self.tkn_m = MWETokenizer(separator=' ')\n",
    "        \n",
    "        # Stopwords to be removes (from stop_words, nltk.corpus and manually inputed)\n",
    "        stop_words = list(stopwords.words('english')) # Contains nt, m, s, d, etc...\n",
    "        extra_words = (\"'\",',','\"','.',']','.[','[','\",','(','\".[','.\"[',':','][',')',';','\".',\"—\",'←','→','•',\n",
    "                      '\"[','),','.\"\"[',',\"', \"’\", \"–\", \"’’\", \"''\",)\n",
    "        stop_words.extend(extra_words)\n",
    "        self.stopwords = stop_words\n",
    "        \n",
    "        # Word processing tool (lemmatizing)\n",
    "        self.lem = WordNetLemmatizer()\n",
    "        \n",
    "    \n",
    "    def read_data(self, path: str) -> list:\n",
    "        \"\"\"\n",
    "        Read files from a directory and then append the data of each file into a list.\n",
    "        \"\"\"\n",
    "        files = PlaintextCorpusReader(path, \".*.txt\")\n",
    "        data_list = list()\n",
    "        # This is for sorting the Simpsons episodes in order\n",
    "        docIDs = []\n",
    "        list_ids = [x.split('.') for x in files.fileids()]\n",
    "        list_ids = sorted(list_ids, key=lambda x: (x[0],int(x[1])))\n",
    "        for i in range(len(list_ids)):\n",
    "            ex = \".\".join(list_ids[i])\n",
    "            docIDs.append(ex)\n",
    "        # Put data from files in a nested list - first term is the id and second is the text\n",
    "        for i in docIDs:\n",
    "            fixed_raw = files.raw(i).replace(\"’\", \"'\")  # Replace missleading \"’\" in data to \"'\" (apostrophe)\n",
    "            fixed_raw = fixed_raw.replace(\"–\", \"-\")     # Replace missleading \"–\" in data to \"-\" (hyphen)\n",
    "            data_list.append([i[:-4], fixed_raw])\n",
    "        \n",
    "        locations_csv = self.read_csv(path, 'simpsons_locations.csv')   # List of lists for locations\n",
    "        characters_csv = self.read_csv(path, 'simpsons_characters.csv') # List of lists for characters\n",
    "        for i, v in enumerate(characters_csv):         # Correct simpsons_characters.csv \"'s\" normalization\n",
    "            if \"'s\" in characters_csv[i][1]:\n",
    "                characters_csv[i][2] = characters_csv[i][1].replace(\"'s\", \"\").lower()\n",
    "        \n",
    "        # Fill the 2 lists (locations and characters) with data from the csv files\n",
    "        # Tokenize data first and also add it to the MWEs for the MWETokenizer()\n",
    "        for i, n, _ in locations_csv:\n",
    "            item = self.tokenization(n, self.tkn_p, option=\"casefold+stopwords\")\n",
    "            self.tkn_m.add_mwe(item)\n",
    "            self.locations.extend(self.tkn_m.tokenize(item))\n",
    "        for i, n, _, _ in characters_csv:\n",
    "            item = self.tokenization(n, self.tkn_p, option=\"casefold+stopwords\")\n",
    "            self.tkn_m.add_mwe(item)\n",
    "            self.characters.extend(self.tkn_m.tokenize(item))\n",
    "        \n",
    "        return data_list\n",
    "\n",
    "    def process_document(self, document: str) -> list:\n",
    "        \"\"\"\n",
    "        Pre-process a document and return a list of its terms\n",
    "        str->list\n",
    "        \"\"\"\n",
    "        docID = document[0]\n",
    "        raw_doc = document[1]\n",
    "        # Update the inverted index documents dictionary with the raw data\n",
    "        self.documents[docID] = raw_doc\n",
    "        \n",
    "        # The tokenization of the text string\n",
    "        # 2 options: \"casefold\" or \"casefold+stopwords\"\n",
    "        terms = self.tokenization(self.documents[docID], self.tkn_p, option=\"casefold+stopwords\")\n",
    "        # Applying the Multi-Word Expression to the tokens\n",
    "        terms_mwe = self.tkn_m.tokenize(terms)\n",
    "        \n",
    "        return terms_mwe\n",
    "    \n",
    "    def index_corpus(self, documents: list) -> None:\n",
    "        \"\"\"\n",
    "        Index given documents\n",
    "        list->None\n",
    "        \"\"\"\n",
    "        # Index given documents in the dictionaries of the inverted index\n",
    "        for data in documents:\n",
    "            docID = data[0]\n",
    "            terms_final = self.process_document(data)\n",
    "            # Update the document frequency\n",
    "            self.inverted_index_docfreq.update(terms_final)\n",
    "            self.token_sequence[docID] = list()\n",
    "            for term in terms_final:\n",
    "                # Add the term in the postings if it is not already there\n",
    "                if term not in list(self.inverted_index_postings.keys()):\n",
    "                    self.inverted_index_postings[term] = dict(dict())\n",
    "                    for i in documents:\n",
    "                        self.inverted_index_postings[term][i[0]] = 0\n",
    "                # Update the inverted index postings\n",
    "                self.inverted_index_postings[term][docID] += 1\n",
    "                # Update the token sequence\n",
    "                self.token_sequence[docID].append(term)\n",
    "        \n",
    "        return None\n",
    "     \n",
    "    def proximity_search(self, term1: str, term2: str, option=\"proxy\") -> dict:\n",
    "        \"\"\"\n",
    "        1) check whether given two terms appear within a window\n",
    "        2) calculate the number of their co-existance in a document\n",
    "        3) add the document id and the number of matches into a dict\n",
    "        return the dict\n",
    "        \"\"\"\n",
    "        dictionary = dict()\n",
    "        window_size = 3 # How far to look to the left and right of a term for the \"proxy\" option\n",
    "        ngram_size = 4  # Size of ngrams for the \"ngrams\" option\n",
    "        # Do the proximity search by finding the term then making a list with its proximity\n",
    "        if option==\"proxy\":\n",
    "            for docID in self.token_sequence.keys():\n",
    "                counter = 0\n",
    "                for i, t in enumerate(self.token_sequence[docID]):\n",
    "                    proximity = []\n",
    "                    if t == term1:\n",
    "                        for x in range(window_size):\n",
    "                            proximity.append(self.token_sequence[docID][i-(x+1)])\n",
    "                            proximity.append(self.token_sequence[docID][i+(x+1)])\n",
    "                    if term2 in proximity:\n",
    "                        counter += 1\n",
    "                        dictionary[docID] = counter\n",
    "        # Do the proximity search by making n-grams with the tokens and looking through them\n",
    "        elif option==\"ngrams\":\n",
    "            for docID in self.token_sequence.keys():\n",
    "                counter = 0\n",
    "                n_grams = ngrams(self.token_sequence[docID], ngram_size)\n",
    "                for g in n_grams:\n",
    "                    if term1 in g and term2 in g:\n",
    "                        counter += 1\n",
    "                        dictionary[docID] = counter\n",
    "        return dictionary\n",
    "        \n",
    "        \n",
    "    def tokenization(self, document, tkn, option=\"\") -> list:\n",
    "        \"\"\"\n",
    "        Tokenization method with multiple options\n",
    "        of tokenization depending on the option input\n",
    "        also contains lemmatization and stemming if need be\n",
    "        \"\"\"\n",
    "        terms = tkn.tokenize(document)\n",
    "        # Just casefolding\n",
    "        if option == \"casefold\":\n",
    "            terms_lower = [t.lower() for t in terms]\n",
    "            terms = terms_lower\n",
    "        # Casefolding and removing stopwords\n",
    "        elif option == \"casefold+stopwords\":\n",
    "            terms_stop = []\n",
    "            temp_term = \"\"\n",
    "            for t in terms:\n",
    "                # Connect the words with a hyphen (-) in between them\n",
    "                if t.lower() == \"-\" or (len(terms_stop) > 0 and terms_stop[-1][-1] == \"-\"):\n",
    "                    if temp_term != \"\":\n",
    "                        terms_stop.append(temp_term + \"-\")\n",
    "                        temp_term = \"\"\n",
    "                        continue\n",
    "                    terms_stop[-1] = terms_stop[-1] + t.lower()\n",
    "                    continue\n",
    "                # If word not in stopwords then add it to final terms list\n",
    "                elif t.lower() not in self.stopwords:\n",
    "                    terms_stop.append(t.lower())\n",
    "                    temp_term = \"\"\n",
    "                # Make sure to also keep the tokens with a stopword but connected by a hyphen\n",
    "                else:\n",
    "                    temp_term = t.lower()\n",
    "            terms = terms_stop\n",
    "            \n",
    "        # Lemmatizing\n",
    "        terms_final = self.tokenize_lem_stm(terms, option=\"lem\")\n",
    "        \n",
    "        return terms_final\n",
    "    \n",
    "    def tokenize_lem_stm(self, terms, lem=WordNetLemmatizer(), stm=SnowballStemmer(\"english\"), option=\"\") -> list:\n",
    "        \"\"\"\n",
    "        Method for lemmatization or stemming or both\n",
    "        \"\"\"\n",
    "        terms_final = []\n",
    "        # Lemmatization\n",
    "        if option == \"lem\":\n",
    "            terms_final = [lem.lemmatize(t) for t in terms]\n",
    "            return terms_final\n",
    "        return terms\n",
    "    \n",
    "    def read_csv(self, path, file_name):\n",
    "        \"\"\"\n",
    "        Method to read from a csv file\n",
    "        \"\"\"\n",
    "        file = open(path + file_name)\n",
    "        csvreader = csv.reader(file)\n",
    "        next(csvreader)\n",
    "        data = [row for row in csvreader]\n",
    "        file.close()\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: Santa's Little Helper\n",
      "Tokenized for searching as: ['santa little helper']\n",
      "You entered a: Character\n",
      "\n",
      "QUERY SEARCH: Appears as a term/token (santa little helper) in:\n",
      "7 episodes ['3.2', '3.4', '3.11', '3.16', '3.19', '3.22', '4.5']\n",
      "1 times in episode: 3.2\n",
      "1 times in episode: 3.4\n",
      "1 times in episode: 3.11\n",
      "1 times in episode: 3.16\n",
      "28 times in episode: 3.19\n",
      "1 times in episode: 3.22\n",
      "1 times in episode: 4.5\n",
      "34 times in total\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"main call function\"\n",
    "    char = False                          # bool to work around location and character names\n",
    "    loc = False\n",
    "    worked = False\n",
    "    index = InvertedIndex()               # initilaise the index\n",
    "    corpus = index.read_data('Simpsons/') # specify the directory path in which files are located\n",
    "    index.index_corpus(corpus)            # index documents/corpus\n",
    "    \n",
    "    search_term = str()\n",
    "    search_term_original = input(\"Enter your query: \") # insert a query\n",
    "    \n",
    "    # Tokenize search term for better search in the inverted index\n",
    "    search_term_list = index.tokenization(search_term_original, index.tkn_p, option=\"casefold+stopwords\")\n",
    "    search_terms = index.tkn_m.tokenize(search_term_list)\n",
    "    print(\"Tokenized for searching as: \" + str(search_terms))\n",
    "    \n",
    "    # Checking if entered terms are characters or locations\n",
    "    for n in index.characters:\n",
    "        if n in search_terms:\n",
    "            print(\"You entered a: Character\")\n",
    "            char = True\n",
    "            break\n",
    "    for n in index.locations:\n",
    "        if n in search_terms:\n",
    "            print(\"You entered a: Location\")\n",
    "            loc = True\n",
    "            break\n",
    "    # Search for 1 term\n",
    "    if len(search_terms) == 1:\n",
    "        search_term = search_terms[0]\n",
    "        print(\"\\nQUERY SEARCH: Appears as a term/token (\"+search_term+\") in:\")\n",
    "        \n",
    "        # Calculate apperance of the term\n",
    "        apperances = dict()\n",
    "        for t in index.inverted_index_postings.keys():\n",
    "            if t == search_term:\n",
    "                for i in index.inverted_index_postings[t].keys():\n",
    "                    if index.inverted_index_postings[t][i] != 0:\n",
    "                        apperances[i] = index.inverted_index_postings[t][i]\n",
    "        \n",
    "        # Display apperance of the term\n",
    "        print(str(len(apperances.keys())) + \" episodes \" + str(list(apperances.keys())))\n",
    "        for k in apperances.keys():\n",
    "            print(str(apperances[k]) + \" times in episode: \" + str(k))\n",
    "        print(str(sum(apperances.values())) + \" times in total\")\n",
    "        worked = True\n",
    "    # Search proximity of 2 terms\n",
    "    if len(search_terms) == 2 or ((char == True or loc == True) and len(search_term_list) == 2):\n",
    "        # Check if terms are: (character + term) or (character name with 2 terms)\n",
    "        if len(search_terms) == 2:\n",
    "            print(\"\\nPROXIMITY: Appears as 2 terms (\"+str(search_terms[0])+\") and (\"+str(search_terms[1])+\") in:\")\n",
    "            dictionary = index.proximity_search(search_terms[0], search_terms[1], option=\"proxy\")\n",
    "        else:\n",
    "            print(\"\\nPROXIMITY: Appears as 2 terms (\"+str(search_term_list[0])+\") and (\"+str(search_term_list[1])+\") in:\")\n",
    "            dictionary = index.proximity_search(search_term_list[0], search_term_list[1], option=\"proxy\")\n",
    "            \n",
    "        # Display apperances of the terms\n",
    "        print(str(len(dictionary.keys())) + \" episodes \" + str(list(dictionary.keys())))\n",
    "        for k in dictionary.keys():\n",
    "            print(str(dictionary[k]) + \" times in episode: \" + str(k))\n",
    "        print(str(sum(dictionary.values())) + \" times in total\")\n",
    "        worked = True\n",
    "            \n",
    "    if worked == False:\n",
    "        print(\"Unrecognized term/terms or too many terms entered\")\n",
    "    return index\n",
    "    \n",
    "index = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
